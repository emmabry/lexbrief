{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sample dataset using first 10k entries to explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              speech  vote   party  \\\n",
      "0  The right hon Gentleman has recited a catalogu...     0  labour   \n",
      "1  I am not sure whether this has occurred to the...     0  labour   \n",
      "2  Before the right hon Gentleman leaves the subj...     0  labour   \n",
      "3  I thank the right hon Member for Penrith and T...     0  labour   \n",
      "4  I thank my right hon Friend for giving way and...     0  labour   \n",
      "\n",
      "   motion_party  policy_preference  \n",
      "0  conservative              305.1  \n",
      "1  conservative              305.1  \n",
      "2  conservative              305.1  \n",
      "3  conservative              305.1  \n",
      "4  conservative              305.1  \n",
      "vote\n",
      "1    5085\n",
      "0    4915\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load just 10k rows to test\n",
    "df = pd.read_csv(\"../data/ParlVote+_raw.csv\", nrows=10000)\n",
    "\n",
    "# Keep only relevant columns\n",
    "df = df[['speech', 'vote', 'party', 'motion_party', 'policy_preference']]\n",
    "\n",
    "print(df.head())\n",
    "print(df['vote'].value_counts())\n",
    "\n",
    "df.to_csv(\"../data/parlvote_sample_10k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data for speech length to get an idea what the input size will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speech length: 763.11 words\n"
     ]
    }
   ],
   "source": [
    "df_sample = pd.read_csv(\"../data/parlvote_sample_10k.csv\")\n",
    "\n",
    "avg_length = df_sample['speech'].apply(lambda x: len(x.split())).mean() \n",
    "\n",
    "print(f\"Average speech length: {avg_length:.2f} words\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average speech length for this sample is 763 words, which is above the maximum token length of 512 that most models have. This was mentioned in the literature.\n",
    "Abercrombie and Batista-Navarro (2022) pad the texts to the maximum input of 512 tokens.\n",
    "Cao & Drinkall (2024) limited the analysis of motions and speeches to a maximum length of 512 tokens, MPNetâ€™s maximum window size.\n",
    "They also mention that \"Future iterations of this work might benefit from exploring alternative strategies to handle longer texts without losing pertinent information.\"\n",
    "\n",
    "### Options:\n",
    "1. Truncation: cut off the speech after the 512 limit. Risks losing important information at the end of debates, but fits most models and has faster training\n",
    "2. Split speeches into 512-token chunks, run model on each, then aggregate outputs (e.g. average predictions)\n",
    "3. Use models designed for longer inputs: LongFormer, BigBird. Could require more setup & compute\n",
    "4. Use a summarisation model first\n",
    "\n",
    "\n",
    "I will start with truncation as this is the simplest and has been used in the two academic papers which use the ParlVote+ dataset. I will use the MPNet tokenizer as it delivered good results in Cao & Drinkall (2024)'s paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "def tokenize_speech(text):\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokens\n",
    "\n",
    "speeches = df_sample['speech'].tolist()\n",
    "\n",
    "tokenized = [tokenize_speech(speech) for speech in speeches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  2519,  2000, 10193, 10174,  5997,  2012,  2032,  2001,  2000,\n",
      "          4058,  3318,  2012,  2007,  2112,  4775,  2003,  2000,  5985,  2007,\n",
      "          2012,  2000,  4108,  3024,  2499,  2295,  3795,  2497,  2017,  2039,\n",
      "          7369,  2001,  2170,  2002,  2017,  2039,  3036,  2004,  3077,  2000,\n",
      "          8910,  2002,  2573,  2795,  2012,  2195,  4108,  5538,  2065, 17030,\n",
      "          2002,  3897,  1033,  2519,  2006,  5997,  2012,  2000,  2235,  1009,\n",
      "          1059, 10344,  2101,  3302,  2189,  2000,  2204,  2497,  2044,  2511,\n",
      "          2000,  4108,  2499,  2295,  2053,  2204,  2573, 28130,  1033,  2070,\n",
      "          3075,  2846,  2044,  2042,  3859,  2018,  1014,  1049,  2576,  2040,\n",
      "          3378,  2012,  2000, 10193,  2270,  2009, 19376,  2007,  2029,  2003,\n",
      "          2018,  2177,  1016,  2000,  3025,  2042,  2046,  1041,  7075,  2009,\n",
      "          2000,  2235,  2017,  2000,  2204,  2931,  1014,  2002,  2000,  4108,\n",
      "          3164,  2042,  2912,  2017,  2923,  2004,  4792,  1016,  2300,  2055,\n",
      "          2012,  2000,  3281,  2042, 17856,  1015, 13437, 21562,  2070, 13415,\n",
      "          2003,  9417, 11237,  2094,  2027,  2002,  2000,  2064,  2177,  1014,\n",
      "          2000,  2235,  1009,  1059, 19781,  2019,  2001,  2041,  2601,  2035,\n",
      "          2472,  2066, 13580,  1016,  2069,  2013,  2024,  2111,  2023,  2594,\n",
      "          3281,  1014,  2000,  3191,  2001,  2114,  2056,  2035,  3859,  2013,\n",
      "          2010,  2427,  2242,  2726,  2047,  2006,  5842,  2004,  2000,  6207,\n",
      "          2079,  9938,  1016,  2178,  1014,  2006,  2110,  2029,  5140,  2000,\n",
      "          8469,  2009,  4108,  2180,  1015,  2099,  5449,  2016,  2012,  2055,\n",
      "          1016, 14603,  1014,  2000,  2708,  2009,  2499,  2002,  3072,  1014,\n",
      "          4108,  2440,  1014,  2005,  2010,  2196,  2004,  2397,  2045,  2000,\n",
      "          3191,  2001,  2114,  1016,  2000,  2708,  2627,  2012,  2049,  2056,\n",
      "          2026,  1041, 16431,  2009,  4108,  2497,  2044,  2024, 14387, 28779,\n",
      "          3713,  2003,  3889,  2002,  2012,  2041,  2963,  1015,  2099,  9887,\n",
      "          2056,  2026,  3829,  1016,  2013,  2005,  2063,  4201,  2045,  2012,\n",
      "          2012,  2005,  2029,  2208,  2442,  2142,  2000, 16431,  3066, 12491,\n",
      "          2001,  2651,  2379,  2002,  2000,  9887,  2001,  2039,  2651,  2497,\n",
      "          2056,  2035,  2004,  2026,  3829,  1016,  2065,  2012, 16431,  2005,\n",
      "          2085,  1016,  2061,  2063,  2022,  2000,  8352, 15538,  3667,  2010,\n",
      "          1041,  2561,  1022,  4750,  2047,  2000,  2708, 15127,  2000,  2759,\n",
      "          2012,  2417,  2497,  2016,  4108,  5538,  2056,  2035,  2041,  9887,\n",
      "          3829,  1014,  2015,  3042,  2012,  2031,  2024,  2116,  2001,  2000,\n",
      "          2651,  2590,  1521,  2001,  2611,  1014,  2000,  9887,  2001,  2497,\n",
      "          2017,  2567,  1014,  2646,  3167,  2034,  3579,  2056,  2029,  2026,\n",
      "          3829,  2142,  2031,  2024,  2116,  2001,  2000,  2651,  2590,  1016,\n",
      "          2148,  2063,  1014,  2000,  2708,  2042,  4932, 13052,  2139,  4043,\n",
      "          2004, 13988,  2014,  8632,  2003,  2000,  2537,  2009,  2499,  2002,\n",
      "          6111,  2004,  4640,  2220,  2497,  2017,  2567,  1014,  3579,  2002,\n",
      "          2646,  3167,  2044,  5467,  4108,  5538,  1016,  4825,  1014,  2000,\n",
      "          5985,  2007,  2059,  2032,  2708,  2187,  2129,  2010,  2014,  2223,\n",
      "          1016,  2416,  2148,  2063,  1014,  2000,  2235,  2035,  2085,  1041,\n",
      "          4168,  2102,  3951,  2004,  3108,  2041,  3166,  2002,  2029,  2511,\n",
      "          2602,  1016,  2939,  7371,  8549,  2144,  1521,  2044,  2042,  2593,\n",
      "          1041,  9491,  3109,  7165,  2004,  6989,  2000,  2235,  1009,  1059,\n",
      "          2601,  2003,  2182,  2177,  1521,  4545,  2012,  1014,  2142,  2049,\n",
      "          2005,  1041,  4493,  2094,  2000,  2052,  3005,  1014,  2000,  3517,\n",
      "          2022,  2004,  2026,  2371,  1016,  2006,  2063,  4918,  2012,  2049,\n",
      "          2056,  2026,  1041,  2239, 28689,  2002,  2798,  2012,  1014,  2008,\n",
      "          2497,  2056,  2026,  7083,  2066,  2009,  2041,  6036,  1014,  2023,\n",
      "          4473,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[1006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./MPNet_tokenizer/tokenizer_config.json',\n",
       " './MPNet_tokenizer/special_tokens_map.json',\n",
       " './MPNet_tokenizer/vocab.txt',\n",
       " './MPNet_tokenizer/added_tokens.json',\n",
       " './MPNet_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./MPNet_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

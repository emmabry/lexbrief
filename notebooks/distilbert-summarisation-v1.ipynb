{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config - paper used BERT & DistilBERT\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "# Load dataset\n",
    "class ExtractiveDataset(Dataset):\n",
    "    def __init__(self, source_file, labels_file, tokenizer, max_len=256):\n",
    "        self.samples = []\n",
    "        with open(source_file) as f_src, open(labels_file) as f_lbl:\n",
    "            for doc, lbl in zip(self._split_docs(f_src), self._split_labels(f_lbl)):\n",
    "                for sent, label in zip(doc, lbl):\n",
    "                    self.samples.append((sent, label))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _split_docs(self, f):\n",
    "        doc, docs = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if doc: docs.append(doc); doc = []\n",
    "            else:\n",
    "                doc.append(line)\n",
    "        if doc: docs.append(doc)\n",
    "        return docs\n",
    "\n",
    "    def _split_labels(self, f):\n",
    "        return [list(map(int, line.strip().split())) for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent, label = self.samples[idx]\n",
    "        enc = self.tokenizer(\n",
    "            sent,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long) \n",
    "        }\n",
    "\n",
    "# Load model + tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# Set up DataLoader\n",
    "train_ds = ExtractiveDataset(\n",
    "    source_file=\"./textrank_train.source\",\n",
    "    labels_file=\"./textrank_oracle_labels.txt\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_dl) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device) \n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

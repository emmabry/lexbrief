{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets nltk rouge-score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# === Load your DistilBERT extractive model ===\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "distilbert_model_path = \"emmabry/disilbert-eurlex\"\n",
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained(distilbert_model_path).to(DEVICE)\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(distilbert_model_path)\n",
    "distilbert_model.eval()\n",
    "\n",
    "def distilbert_extract(text, max_tokens=1024):\n",
    "    sentences = sent_tokenize(text)\n",
    "    selected_sents = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        inputs = distilbert_tokenizer(sent, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = distilbert_model(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        if pred == 1:\n",
    "            tokens = len(distilbert_tokenizer(sent)[\"input_ids\"])\n",
    "            if current_tokens + tokens <= max_tokens:\n",
    "                selected_sents.append(sent)\n",
    "                current_tokens += tokens\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return \" \".join(selected_sents)\n",
    "\n",
    "# === Custom Dataset using distilbert_extract ===\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, source_file, target_file, tokenizer, max_source_len=1024, max_target_len=128):\n",
    "        self.sources = []\n",
    "        self.targets = []\n",
    "        with open(source_file, encoding='utf-8') as f_src, open(target_file, encoding='utf-8') as f_tgt:\n",
    "            for src_line, tgt_line in zip(f_src, f_tgt):\n",
    "                self.sources.append(src_line.strip())\n",
    "                self.targets.append(tgt_line.strip())\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_len = max_source_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_source = self.sources[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Apply extractive summarisation\n",
    "        extracted = distilbert_extract(raw_source)\n",
    "\n",
    "        # Tokenize\n",
    "        source_enc = self.tokenizer(\n",
    "            extracted,\n",
    "            max_length=self.max_source_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target_enc = self.tokenizer(\n",
    "            target,\n",
    "            max_length=self.max_target_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = target_enc['input_ids'].squeeze()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100  # ignore pad tokens in loss\n",
    "\n",
    "        return {\n",
    "            'input_ids': source_enc['input_ids'].squeeze(),\n",
    "            'attention_mask': source_enc['attention_mask'].squeeze(),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# === Model, Tokenizer and Hyperparameters ===\n",
    "MODEL_NAME = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "# === Load dataset ===\n",
    "train_ds = SummarizationDataset('train.source', 'train.target', tokenizer)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# === Optimizer & Scheduler ===\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_dl) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# === Training Loop ===\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
